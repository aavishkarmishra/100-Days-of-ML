# 100-Days-of-ML

Joined udacity's [Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120) course and this repository is to keep my projects and records of this course.

## Naive Bayes

>It is classification algorithm which the order of objects doesn't matter.

>>Using this algorithm I have performed a mini project  
>>[Naive Bayes](/naive_bayes/nb_author_id.py) using **sklearn.naive_bayes** and got **accuracy of 97.042%**.

### Output of Naive Bayes

![Naive Bayes Output](/naive_bayes/nb_author_id_output.jpg "Naive Bayes Output")

## Support Vector Machine  

>It is another classification algorithm . This algorithm **is much slower than**  
>**Naive Bayes** but we can improve speed by reducing the training data size
>but this will decreases the accuracy.

>>Using this algorithm I have performed a mini project  
>>[SVM](/svm/svm_author_id.py) using **sklearn's svm** and got **accuracy of 99.345%**.

### Output of SVM

![SVM Output](/svm/svm_author_id_output.jpg "SVM Output")

## Decision Trees

>It also a classification algorithm but it is also used for regression.  
>It have several parameters for tuning.  
>In decision trees there are some formula for calulating some of them  
>**Entropy** is the number of impurity or is equal to `sum of(-(pi*log(pi,2))`  
>**Information Gain** is equal to `Entropy(parent)-(weighted average)Entropy(chidlren)`  
> Decision Trees algorithm tries to **maximize the Information gain** and **minimize the Entropy**
>>Using this algorithm I have performed a mini project  
>>[decision_tree](/decision_tree/dt_author_id.py) using **sklearn's decision tree** and got **accuracy of 97.923%**.

### Output of Decision Tree

![Decision Tree Output](/decision_tree/dt_author_id_output.jpg "Decision Tree Output")

## Choose Your Own 

### K nearest neighbors

>In supervised learning, this algorithm uses user-defined no of  
>neighbors to give the lable of a new point. This algorithm is used  
> for both classification and regression.

### AdaBoost(**Ada**ptive **Boost**ing)
>AdaBoost (Adaptive Boosting) is a very popular boosting technique  
that aims at combining multiple weak classifiers to build one strong classifier.  
>*For detailed information* [AdaBoost](https://blog.paperspace.com/adaboost-optimizer/#:~:text=AdaBoost%20is%20an%20ensemble%20learning,turn%20them%20into%20strong%20ones.)
### Random Forest
>Random forests or random decision forests are an ensemble learning method for classification,  
>regression and other tasks that operate by constructing a multitude of decision trees at training time and  
>outputting the class that is the mode of the classes (classification) or mean prediction (regression)  
>of the individual trees.

>>Using these algorithms I have performed a mini project  
>>[choose_your_own](/choose_your_own) using **sklearn** and got **accuracy of 94% (K-nearest neighbors), 92,4% (AdaBoost) & 92.4% (Random Forest)**.


### Output of Choose_your_own

![Choose_your_oen Output](/choose_your_own/your_algorithm_output.jpg "Choose your own Output")

<hr>

## Author

### -[Aavishkar Mishra](https://github.com/aavishkarmishra)
[<img src="https://image.flaticon.com/icons/svg/185/185964.svg" width="35" padding="10">](https://www.linkedin.com/in/aavishkarmishra/)
[<img src="https://image.flaticon.com/icons/svg/185/185981.svg" width="35" padding="10">](https://www.facebook.com/aavishkarmishra)
[<img src="https://image.flaticon.com/icons/svg/185/185985.svg" width="35" padding="10">](https://www.instagram.com/aavishkar_mishra/)
